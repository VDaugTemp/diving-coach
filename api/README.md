# Diving Coach API

FastAPI backend for diving instruction with RAG (Retrieval Augmented Generation).

## Setup

```bash
pip install openai fastapi uvicorn pypdf python-multipart numpy
export OPENAI_API_KEY="your-key-here"
```

## Run

```bash
python app.py
```

Server starts at `http://localhost:8000`

Documents from `data/` folder are automatically loaded at startup.

## API Endpoints

- `POST /api/chat` - Chat with streaming response
- `GET /api/health` - Health check
- `GET /api/ingest/stats` - Vector store statistics
- `POST /api/ingest/reload` - Reload documents from data/

## Project Structure

```
api/
â”œâ”€â”€ app.py              # Main FastAPI application
â”œâ”€â”€ ingest.py           # Document loading and ingestion
â”œâ”€â”€ vector_store.py     # In-memory vector database
â”œâ”€â”€ embeddings.py       # OpenAI embeddings
â”œâ”€â”€ similarity.py       # Cosine similarity calculations
â”œâ”€â”€ loaders.py          # Text/PDF loaders and chunking
â”œâ”€â”€ test_vector_store.py # Tests
â”œâ”€â”€ ingest_data.py      # Utility script for testing
â””â”€â”€ data/               # Diving manuals (PDFs/TXT)
```

## Testing

```bash
# Test vector store
python test_vector_store.py

# Test document ingestion
python ingest_data.py
```

## How It Works

1. **Startup**: Loads all `.txt` and `.pdf` files from `data/` folder
2. **Chunking**: Splits documents into 1000-char chunks (200 overlap)
3. **Embeddings**: Generates vectors using OpenAI's text-embedding-3-small
4. **Storage**: Stores in in-memory vector database (687 chunks from 6 manuals)
5. **Search**: Cosine similarity for semantic search (coming in Phase 5)

## Current Status

âœ… Document ingestion (Phase 4)
âœ… Vector storage with semantic search
âœ… Chat endpoint with streaming
âœ… RAG integration (Step 10)
âœ… Prompt templates (Step 11)
ðŸŽ¯ Ready for production testing
